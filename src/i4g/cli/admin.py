"""Administrative utilities for the i4g platform."""

from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
import warnings
from collections.abc import Sequence as AbcSequence
from pathlib import Path
from typing import Any

from google.cloud import discoveryengine_v1beta as discoveryengine
from google.protobuf import json_format
from rich.console import Console

from i4g.services.factories import build_review_store, build_vector_store
from i4g.settings import get_settings

# Fix for OpenMP runtime conflict on macOS.
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

console = Console()
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

SETTINGS = get_settings()


def _convert_struct(data: Any) -> Any:
    """Recursively convert proto-plus map/repeated containers into builtin types."""

    if isinstance(data, (str, int, float, bool)) or data is None:
        return data
    if hasattr(data, "items"):
        return {key: _convert_struct(value) for key, value in data.items()}
    if isinstance(data, AbcSequence) and not isinstance(data, (str, bytes, bytearray)):
        return [_convert_struct(value) for value in data]
    return data


def ensure_ollama_running() -> bool:
    """
    Quick connectivity check for Ollama.
    Returns True if the Ollama daemon is reachable, False otherwise.
    """
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=3)
        return result.returncode == 0
    except Exception:
        return False


def run_query(args: argparse.Namespace) -> None:
    from i4g.rag.pipeline import build_scam_detection_chain

    if not ensure_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Start it first with:[/red]")
        console.print("    ollama serve\n")
        sys.exit(1)

    console.print("[green]‚úÖ Ollama detected. Loading vectorstore...[/green]")

    try:
        store = build_vector_store(backend=args.backend)
    except Exception as e:  # pragma: no cover - defensive logging
        console.print(f"[red]Failed to initialize vectorstore:[/red] {e}")
        console.print("Make sure you ran `python scripts/build_index.py` successfully.")
        sys.exit(1)

    console.print("[cyan]üîó Building scam detection chain...[/cyan]")
    chain = build_scam_detection_chain(store)

    console.print(f"[cyan]ü§ñ Analyzing question:[/cyan] {args.question}\n")

    try:
        result = chain.invoke({"question": args.question})
    except Exception as e:  # pragma: no cover - defensive logging
        console.print(f"[red]‚ùå Query failed:[/red] {e}")
        sys.exit(1)

    console.print("\n[bold green]üß† Scam Detection Result:[/bold green]")
    console.print(result)

    console.print("\n[dim]Note: Results are generated by a local LLM using RAG context retrieval.[/dim]")


def run_vertex_search(args: argparse.Namespace) -> None:
    project = args.project or SETTINGS.vector.vertex_ai_project
    if not project:
        console.print("[red]‚ùå Provide --project or set I4G_VECTOR__VERTEX_AI__PROJECT.[/red]")
        sys.exit(1)

    location = args.location or SETTINGS.vector.vertex_ai_location or "global"
    data_store_id = args.data_store_id
    if not data_store_id:
        console.print("[red]‚ùå --data-store-id is required.[/red]")
        sys.exit(1)

    client = discoveryengine.SearchServiceClient()
    serving_config = client.serving_config_path(
        project=project,
        location=location,
        data_store=data_store_id,
        serving_config=args.serving_config_id,
    )

    request = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=args.query,
        page_size=args.page_size,
    )

    if args.filter_expression:
        request.filter = args.filter_expression

    if args.boost_json:
        try:
            boost_payload = json.loads(args.boost_json)
        except json.JSONDecodeError as exc:
            console.print(f"[red]‚ùå Failed to parse --boost-json:[/red] {exc}")
            sys.exit(1)

        boost_spec = discoveryengine.SearchRequest.BoostSpec()
        json_format.ParseDict(boost_payload, boost_spec._pb)
        request.boost_spec = boost_spec

    console.print(
        f"[cyan]üîç Vertex search:[/cyan] project={project} location={location} data_store={data_store_id} query='{args.query}'"
    )

    try:
        results = list(client.search(request=request))
    except Exception as exc:  # pragma: no cover - network failure
        console.print(f"[red]‚ùå Search failed:[/red] {exc}")
        sys.exit(1)

    if args.raw:
        payload = [json_format.MessageToDict(result._pb) for result in results]  # type: ignore[attr-defined]
        console.print_json(data=json.dumps(payload))
        return

    if not results:
        console.print("[yellow]No results returned.[/yellow]")
        return

    for rank, result in enumerate(results, start=1):
        document = result.document
        struct: dict[str, Any] = {}
        if document.json_data:
            try:
                struct = json.loads(document.json_data)
            except json.JSONDecodeError:
                struct = _convert_struct(document.struct_data) if document.struct_data else {}
        elif document.struct_data:
            struct = _convert_struct(document.struct_data)

        summary = struct.get("summary") or document.title or "<no summary>"
        label = struct.get("ground_truth_label") or "<unknown>"
        tags = ", ".join(struct.get("tags") or [])

        console.print(f"[bold cyan]#{rank}[/bold cyan] id={document.id} label={label}")
        if tags:
            console.print(f"    [dim]tags:[/dim] {tags}")
        console.print(f"    {summary}\n")


def export_saved_searches(args: argparse.Namespace) -> None:
    """Dump saved searches to JSON."""
    store = build_review_store()
    owner_filter = None if args.all else (args.owner or None)
    records = store.list_saved_searches(owner=owner_filter, limit=args.limit)
    include_tags = set(t.strip().lower() for t in (args.include_tags or []))
    if include_tags:
        records = [r for r in records if include_tags.intersection({t.lower() for t in (r.get("tags") or [])})]
    for record in records:
        record.pop("created_at", None)
        if record.get("tags") is None:
            record["tags"] = []
    if args.split and args.output:
        base = Path(args.output)
        base.mkdir(parents=True, exist_ok=True)
        by_owner: dict[str, list[dict[str, object]]] = {}
        for record in records:
            owner = record.get("owner") or "shared"
            by_owner.setdefault(owner, []).append(record)
        for owner, rows in by_owner.items():
            fname = base / f"saved_searches_{owner}.json"
            fname.write_text(json.dumps(rows, indent=2))
            console.print(f"[green]‚úÖ Exported {len(rows)} saved search(es) to {fname}")
    else:
        data = json.dumps(records, indent=2)
        if args.output:
            Path(args.output).write_text(data)
            console.print(f"[green]‚úÖ Exported {len(records)} saved search(es) to {args.output}")
        else:
            console.print(data)


def import_saved_searches(args: argparse.Namespace) -> None:
    """Load saved searches from JSON file/stdin."""
    from i4g.api.review import SavedSearchImportRequest

    store = build_review_store()
    content = Path(args.input).read_text() if args.input else sys.stdin.read()
    try:
        payload = json.loads(content)
    except json.JSONDecodeError as exc:
        console.print(f"[red]‚ùå Invalid JSON:[/red] {exc}")
        sys.exit(1)

    include_tags = set(t.strip().lower() for t in (args.include_tags or []))
    items = payload if isinstance(payload, list) else [payload]
    imported = 0
    skipped = 0
    for item in items:
        try:
            req = SavedSearchImportRequest(**item)
            if include_tags and not include_tags.intersection({t.lower() for t in (req.tags or [])}):
                skipped += 1
                continue
            store.import_saved_search(req.model_dump(), owner=None if args.shared else args.owner)
            imported += 1
        except Exception as exc:  # pragma: no cover - defensive logging
            skipped += 1
            console.print(f"[yellow]Skipped #{imported + skipped}: {exc}[/yellow]")
    console.print(f"[green]‚úÖ Imported {imported} saved search(es); {skipped} skipped.")


def prune_saved_searches(args: argparse.Namespace) -> None:
    store = build_review_store()
    records = store.list_saved_searches(owner=args.owner, limit=1000)
    tags_filter = set(t.strip().lower() for t in (args.tags or []))
    to_delete = []
    for record in records:
        tags = {t.lower() for t in (record.get("tags") or [])}
        if tags_filter and not tags_filter.intersection(tags):
            continue
        to_delete.append(record)

    if not to_delete:
        console.print("[yellow]No saved searches matched the criteria.")
        return

    for record in to_delete:
        owner = record.get("owner") or "shared"
        console.print(f"[cyan]- {record.get('name')} (owner={owner}, tags={record.get('tags')})")

    if args.dry_run:
        console.print(f"[green]Dry run: {len(to_delete)} saved search(es) would be deleted.")
        return

    deleted = 0
    for record in to_delete:
        if store.delete_saved_search(record["search_id"]):
            deleted += 1
    console.print(f"[green]‚úÖ Deleted {deleted} saved search(es).")


def bulk_update_saved_search_tags(args: argparse.Namespace) -> None:
    if not any([args.add, args.remove, args.replace is not None]):
        console.print("[red]Provide --add, --remove, or --replace to adjust tags.[/red]")
        sys.exit(1)

    if args.replace is not None and (args.add or args.remove):
        console.print("[yellow]‚ö†Ô∏è --replace overrides --add/--remove; add/remove values will be ignored.[/yellow]")

    store = build_review_store()
    normalized_add = [t.strip() for t in (args.add or []) if t.strip()]
    normalized_remove = [t.strip() for t in (args.remove or []) if t.strip()]
    normalized_replace = [t.strip() for t in (args.replace or []) if t.strip()] if args.replace is not None else None

    summary_records = []
    target_ids = []

    if args.search_id:
        target_ids = [sid for sid in args.search_id if sid.strip()]
        for sid in target_ids:
            record = store.get_saved_search(sid)
            if record:
                summary_records.append(record)
    else:
        records = store.list_saved_searches(owner=args.owner, limit=args.limit)
        tags_filter = {t.strip().lower() for t in (args.tags or []) if t.strip()}
        if tags_filter:
            records = [r for r in records if tags_filter.intersection({t.lower() for t in (r.get("tags") or [])})]
        summary_records = records
        target_ids = [r["search_id"] for r in records]

    target_ids = list(dict.fromkeys(target_ids))

    if not target_ids:
        console.print("[yellow]No saved searches matched the criteria.")
        return

    if args.search_id:
        found_ids = {record["search_id"] for record in summary_records}
        missing_ids = [sid for sid in target_ids if sid not in found_ids]
        if missing_ids:
            console.print(
                "[yellow]Warning:[/yellow] the following saved search ID(s) were not found and will be skipped: "
                + ", ".join(missing_ids)
            )

    if args.dry_run:
        console.print(f"[green]Dry run:[/green] would update {len(target_ids)} saved search(es).")
        for record in summary_records[:10]:
            owner = record.get("owner") or "shared"
            console.print(f"  - {record.get('name')} (owner={owner}, tags={record.get('tags') or []})")
        if len(summary_records) > 10:
            console.print(f"  ...and {len(summary_records) - 10} more.")
        return

    updated = store.bulk_update_tags(
        target_ids,
        add=normalized_add,
        remove=normalized_remove,
        replace=normalized_replace,
    )
    console.print(f"[green]‚úÖ Updated tags for {updated} saved search(es).")


def export_tag_presets(args: argparse.Namespace) -> None:
    store = build_review_store()
    presets = store.list_tag_presets(owner=args.owner, limit=1000)
    data = json.dumps(presets, indent=2)
    if args.output:
        Path(args.output).write_text(data)
        console.print(f"[green]‚úÖ Exported {len(presets)} tag preset(s) to {args.output}")
    else:
        console.print(data)


def import_tag_presets(args: argparse.Namespace) -> None:
    content = Path(args.input).read_text() if args.input else sys.stdin.read()
    try:
        payload = json.loads(content)
    except json.JSONDecodeError as exc:
        console.print(f"[red]‚ùå Invalid JSON:[/red] {exc}")
        sys.exit(1)
    items = payload if isinstance(payload, list) else [payload]
    presets = []
    for item in items:
        tags = item.get("tags") or []
        if tags and tags not in presets:
            presets.append(tags)
    if not presets:
        console.print("[yellow]No tag presets found in input.")
        return
    output = json.dumps(presets, indent=2)
    if args.input:
        Path(args.input).write_text(output)
        console.print(f"[green]‚úÖ Normalized {len(presets)} tag preset(s).")
    else:
        console.print(output)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="i4g administrative utilities.")
    sub = parser.add_subparsers(dest="command", required=True)

    query = sub.add_parser("query", help="Run scam detection query.")
    query.add_argument("--question", required=True, help="User question text.")
    query.add_argument(
        "--backend",
        type=str,
        default=SETTINGS.vector.backend,
        choices=["chroma", "faiss"],
        help="Vector backend to use (overrides I4G_VECTOR_BACKEND).",
    )
    query.set_defaults(func=run_query)

    vertex = sub.add_parser("vertex-search", help="Query Vertex AI Search (Discovery) data store.")
    vertex.add_argument("query", help="Free-text query string to execute.")
    vertex.add_argument(
        "--project",
        default=SETTINGS.vector.vertex_ai_project,
        help="Google Cloud project containing the Discovery data store.",
    )
    vertex.add_argument(
        "--location",
        default=SETTINGS.vector.vertex_ai_location or "global",
        help="Discovery location (default: global).",
    )
    vertex.add_argument(
        "--data-store-id",
        required=True,
        help="Discovery data store identifier.",
    )
    vertex.add_argument(
        "--serving-config-id",
        default="default_search",
        help="Serving config identifier (default: default_search).",
    )
    vertex.add_argument(
        "--page-size",
        type=int,
        default=5,
        help="Maximum number of results to return (default: 5).",
    )
    vertex.add_argument(
        "--filter",
        dest="filter_expression",
        help="Optional filter expression (Discovery filter syntax).",
    )
    vertex.add_argument(
        "--boost-json",
        help="Optional BoostSpec payload expressed as JSON.",
    )
    vertex.add_argument(
        "--raw",
        action="store_true",
        help="Print raw JSON response instead of a formatted summary.",
    )
    vertex.set_defaults(func=run_vertex_search)

    export = sub.add_parser("export-saved-searches", help="Export saved searches to JSON.")
    export.add_argument("--limit", type=int, default=100, help="Max entries to export.")
    export.add_argument(
        "--all",
        action="store_true",
        help="Include shared searches along with personal ones.",
    )
    export.add_argument("--owner", help="Filter by owner username (ignored if --all).")
    export.add_argument("--output", help="Output file; omit for stdout.")
    export.add_argument(
        "--split",
        action="store_true",
        help="When writing to a folder, create one file per owner.",
    )
    export.add_argument(
        "--include-tags",
        nargs="*",
        help="Only export saved searches matching these tags (case-insensitive).",
    )
    export.set_defaults(func=export_saved_searches)

    imp = sub.add_parser("import-saved-searches", help="Import saved searches from JSON.")
    imp.add_argument("--input", help="JSON file path (defaults to stdin).")
    imp.add_argument(
        "--owner",
        default=None,
        help="Owner username for imported searches (default: current user).",
    )
    imp.add_argument("--shared", action="store_true", help="Import into shared scope (owner=NULL).")
    imp.add_argument(
        "--include-tags",
        nargs="*",
        help="Only import saved searches that include these tags.",
    )
    imp.set_defaults(func=import_saved_searches)

    prune = sub.add_parser("prune-saved-searches", help="Delete saved searches by owner/tag filters.")
    prune.add_argument(
        "--owner",
        help="Delete saved searches belonging to this owner (omit for shared).",
    )
    prune.add_argument(
        "--tags",
        nargs="*",
        help="Only delete saved searches containing any of these tags.",
    )
    prune.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview deletions without applying them.",
    )
    prune.set_defaults(func=prune_saved_searches)

    bulk = sub.add_parser(
        "bulk-update-tags",
        help="Add, remove, or replace saved-search tags in bulk.",
    )
    bulk.add_argument("--owner", help="Filter saved searches to this owner (default: all owners).")
    bulk.add_argument(
        "--tags",
        nargs="*",
        help="Only target saved searches containing these tags (any match).",
    )
    bulk.add_argument(
        "--search-id",
        nargs="*",
        help="Explicit saved search IDs to update (skips owner/tag filters).",
    )
    bulk.add_argument("--add", nargs="+", help="Tags to add to each matched saved search.")
    bulk.add_argument("--remove", nargs="+", help="Tags to remove from each matched saved search.")
    bulk.add_argument(
        "--replace",
        nargs="+",
        help="Replace the existing tag set with this list (overrides --add/--remove).",
    )
    bulk.add_argument(
        "--limit",
        type=int,
        default=200,
        help="Max saved searches to inspect when filtering.",
    )
    bulk.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview the changes without persisting them.",
    )
    bulk.set_defaults(func=bulk_update_saved_search_tags)

    export_tags = sub.add_parser("export-tag-presets", help="Export tag presets derived from saved searches.")
    export_tags.add_argument("--owner", help="Filter presets to this owner (omit for shared).")
    export_tags.add_argument("--output", help="File to write JSON (stdout if omitted).")
    export_tags.set_defaults(func=export_tag_presets)

    import_tags = sub.add_parser("import-tag-presets", help="Import tag presets and append as filter presets.")
    import_tags.add_argument("--input", help="JSON file path (defaults to stdin).")
    import_tags.set_defaults(func=import_tag_presets)

    return parser


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(args=argv)
    args.func(args)


__all__ = [
    "build_parser",
    "main",
    "run_query",
    "run_vertex_search",
]
