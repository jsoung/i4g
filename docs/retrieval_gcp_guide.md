# GCP Retrieval Guide — Vertex AI Search PoC

This guide documents the Google Cloud workflow for the Retrieval proof of concept. It covers the Terraform-provisioned Discovery Engine data store, the Python ingestion and query helpers, and day-to-day troubleshooting. Keep this separate from the local sandbox guide to avoid mixing laptop-only steps with hosted services.

---

## Prerequisites

- `infra/environments/dev` Terraform stack applied (creates the Discovery Engine data store `retrieval-poc`).
- Application Default Credentials available on your workstation with the quota project set: `gcloud auth application-default set-quota-project i4g-dev`.
- Active Python environment `i4g` with the dependencies from `pyproject.toml` installed (`google-cloud-discoveryengine>=0.15,<1`).
- Access to `data/retrieval_poc/cases.jsonl` (generated by the synthetic dataset scripts).

---

## Ingestion Script

Use `scripts/ingest_vertex_search.py` to load the JSONL cases into the hosted data store.

```bash
python scripts/ingest_vertex_search.py \
    --project i4g-dev \
    --location global \
    --data-store-id retrieval-poc \
    --jsonl data/retrieval_poc/cases.jsonl
```

Key flags:

- `--dry-run` — parse the first record and log the protobuf payload without calling Discovery Engine.
- `--batch-size` — tune the number of documents sent per import batch (default 50).
- `--reconcile-mode` — choose between incremental and full replacement (`INCREMENTAL` by default).

What changed recently:

- Each document now sets `Document.content` with UTF-8 raw bytes and `mime_type="text/plain"`, satisfying Discovery Engine’s unstructured content requirement.
- Dry-run logging uses the underlying protobuf message for stable JSON dumps (`proto-plus` wrappers lack a `DESCRIPTOR`).

After a successful run you should see log lines similar to:

```
INFO Batch 1 completed: success=50 failure=0
INFO Ingestion complete: 200 succeeded, 0 failed, total input 200
```

If `error_samples` are reported, rerun with `--verbose` and check the payload for malformed fields.

---

## Query Script

`scripts/query_vertex_search.py` performs ad-hoc searches against the data store. Basic usage:

```bash
python scripts/query_vertex_search.py \
    "wallet address verification" \
    --project i4g-dev \
    --location global \
    --data-store-id retrieval-poc \
    --page-size 5
```

### Filters

Discovery Engine filter syntax is strict about spacing. Arrays such as `tags` require the `ANY` operator **with a space on both sides of the colon**:

```bash
python scripts/query_vertex_search.py \
    "verification" \
    --project i4g-dev \
    --location global \
    --data-store-id retrieval-poc \
    --filter 'tags: ANY("account-security")'
```

If the service returns `Unsupported field "tags" on ":" operator`, double-check the colon spacing.

### Boosting

Rank adjustments can be tested without changing the hosted configuration by passing `--boost-json`:

```bash
python scripts/query_vertex_search.py \
    "verification" \
    --project i4g-dev \
    --location global \
    --data-store-id retrieval-poc \
    --boost-json '{"conditionBoostSpecs": [{"condition": "tags: ANY(\"romance\")", "boost": -0.5}]}'
```

The script JSON-parses the payload and merges it into `SearchRequest.BoostSpec`, allowing quick experiments with positive or negative boosts.

### Raw Output

Add `--raw` to print the full protobuf response as JSON. This is useful for debugging filterable fields (`structData`, `rankSignals`, etc.).

---

## Schema & Filter Reference

The data store currently uses the auto-generated `default_schema`. You can inspect it with:

```bash
python - <<'PY'
from google.cloud import discoveryengine_v1beta as discoveryengine

client = discoveryengine.SchemaServiceClient()
name = "projects/544936845045/locations/global/collections/default_collection/dataStores/retrieval-poc/schemas/default_schema"
print(client.get_schema(name=name).json_schema)
PY
```

Fields such as `tags`, `category`, `structured_fields.asset`, and `entities.crypto_assets.value` are marked `dynamicFacetable` and `searchable`, meaning they support filters and boosts directly.

---

## Troubleshooting Checklist

- **No results** — Verify ingestion succeeded (`error_samples` should be empty) and re-run the query with `--raw` to confirm hits exist.
- **Filter errors** — Ensure `tags: ANY("value")` spacing matches the schema; the parser rejects `tags:"value"`.
- **Boost payload failures** — `--boost-json` must be valid JSON; the script will exit with an explanatory message if parsing fails.
- **Credential issues** — Refresh ADC with `gcloud auth application-default login` and confirm the quota project is set.

---

## Quick Smoke Test

Run the wrapper below before a major checkpoint to confirm both ingestion parsing and hosted search stay healthy:

```bash
python scripts/smoke_vertex_retrieval.py \
    --project i4g-dev \
    --location global \
    --data-store-id retrieval-poc
```

The script performs a dry-run ingest on `data/retrieval_poc/cases.jsonl` and then issues a search against the `default_search` serving config. It exits non-zero if parsing fails or the query returns zero documents, giving you one command to spot regressions.

---

## Next Steps

- Capture favorite filter/boost combinations in versioned JSON snippets for quick reuse.
- Add automated smoke tests (e.g., GitHub Actions job) that performs a dry-run ingest followed by a query using the mocked data store.
- When the schema diverges from the default, export the custom schema to `infra/` so Terraform keeps the configuration synced.
